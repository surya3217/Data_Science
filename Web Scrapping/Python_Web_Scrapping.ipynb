{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scrapping Through existing HTML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scraping with BeautifulSoup and Requests\n",
    "\n",
    "# Parsing the content from the website and \n",
    "# pulling the exact information you want\n",
    "# Introduce to the page for Web Scrapping \n",
    "\n",
    "# pip install beautifulsoup4\n",
    "# pip install lxml\n",
    "# pip install html5lib\n",
    "\n",
    "\"\"\"\n",
    "Introduce the concept of basic HTML tags\n",
    "HTML\n",
    "  HEAD\n",
    "    \n",
    "  HEAD\n",
    "\n",
    "  BODY\n",
    "    \n",
    "  BODY\n",
    "HTML\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Create simple html files and \n",
    "# parse that using bs4 to make the students understand with title, div etc\n",
    "\n",
    "html_file = open(\"mrdoob.html\")    ## existing html file \n",
    "soup = BeautifulSoup(html_file, \"lxml\")   # html5lib is another parser\n",
    "\n",
    "# print (soup)\n",
    "print (soup.prettify())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Mr.doob</title>\n",
      "Mr.doob\n"
     ]
    }
   ],
   "source": [
    "print (soup.title)\n",
    "print (soup.title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div id=\"nav\">\n",
      "<div id=\"logo\">\n",
      "<map name=\"logo\">\n",
      "<area alt=\"Mr.doob\" coords=\"12,13,91,32\" href=\"/\" shape=\"rect\"/>\n",
      "<area alt=\"Blog\" coords=\"12,32,44,46\" href=\"https://ello.co/mrdoob\" shape=\"rect\"/>\n",
      "<area alt=\"Twitter\" coords=\"47,32,88,46\" href=\"https://twitter.com/mrdoob\" shape=\"rect\"/>\n",
      "</map>\n",
      "<img alt=\"logo\" src=\"files/showcase/logo.svg\" usemap=\"#logo\" width=\"105\"/>\n",
      "</div>\n",
      "<div id=\"projects\"></div>\n",
      "<div id=\"expand\"><span></span><span></span><span></span></div>\n",
      "<!-- <a href=\"/blog\"><img src=\"files/showcase/more.png\" width=\"60\" style=\"float:left\" /></a> -->\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "# Crome browser ( use the inspect tool to use the find function )\n",
    "match = soup.find('div')  ## give first occurance of div\n",
    "print (match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "match = soup.find(\"div\", class_= \"footer\")\n",
    "print (match)\n",
    "\n",
    "print ( match.h2 )\n",
    "print ( match.h2.text )\n",
    "print ( match.p )\n",
    "print ( match.p.text )\n",
    "\n",
    "for article in soup.find_all(\"div\"):\n",
    "    headline = article.p.text\n",
    "    print (headline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from the Internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head>\n",
      " </head>\n",
      " <body>\n",
      "  <h1>\n",
      "   Herman Melville - Moby-Dick\n",
      "  </h1>\n",
      "  <div>\n",
      "   <p>\n",
      "    Availing himself of the mild, summer-cool weather that now reigned in these latitudes, and in preparation for the peculiarly active pursuits shortly to be anticipated, Perth, the begrimed, blistered old blacksmith, had not removed his portable forge to the hold again, after concluding his contributory work for Ahab's leg, but still retained it on deck, fast lashed to ringbolts by the foremast; being now almost incessantly invoked by the headsmen, and harpooneers, and bowsmen to do some little job for them; altering, or repairing, or new shaping their various weapons and boat furniture. Often he would be surrounded by an eager circle, all waiting to be served; holding boat-spades, pike-heads, harpoons, and lances, and jealously watching his every sooty movement, as he toiled. Nevertheless, this old man's was a patient hammer wielded by a patient arm. No murmur, no impatience, no petulance did come from him. Silent, slow, and solemn; bowing over still further his chronically broken back, he toiled away, as if toil were life itself, and the heavy beating of his hammer the heavy beating of his heart. And so it was.â€”Most miserable! A peculiar walk in this old man, a certain slight but painful appearing yawing in his gait, had at an early period of the voyage excited the curiosity of the mariners. And to the importunity of their persisted questionings he had finally given in; and so it came to pass that every one now knew the shameful story of his wretched fate. Belated, and not innocently, one bitter winter's midnight, on the road running between two country towns, the blacksmith half-stupidly felt the deadly numbness stealing over him, and sought refuge in a leaning, dilapidated barn. The issue was, the loss of the extremities of both feet. Out of this revelation, part by part, at last came out the four acts of the gladness, and the one long, and as yet uncatastrophied fifth act of the grief of his life's drama. He was an old man, who, at the age of nearly sixty, had postponedly encountered that thing in sorrow's technicals called ruin. He had been an artisan of famed excellence, and with plenty to do; owned a house and garden; embraced a youthful, daughter-like, loving wife, and three blithe, ruddy children; every Sunday went to a cheerful-looking church, planted in a grove. But one night, under cover of darkness, and further concealed in a most cunning disguisement, a desperate burglar slid into his happy home, and robbed them all of everything. And darker yet to tell, the blacksmith himself did ignorantly conduct this burglar into his family's heart. It was the Bottle Conjuror! Upon the opening of that fatal cork, forth flew the fiend, and shrivelled up his home. Now, for prudent, most wise, and economic reasons, the blacksmith's shop was in the basement of his dwelling, but with a separate entrance to it; so that always had the young and loving healthy wife listened with no unhappy nervousness, but with vigorous pleasure, to the stout ringing of her young-armed old husband's hammer; whose reverberations, muffled by passing through the floors and walls, came up to her, not unsweetly, in her nursery; and so, to stout Labor's iron lullaby, the blacksmith's infants were rocked to slumber. Oh, woe on woe! Oh, Death, why canst thou not sometimes be timely? Hadst thou taken this old blacksmith to thyself ere his full ruin came upon him, then had the young widow had a delicious grief, and her orphans a truly venerable, legendary sire to dream of in their after years; and all of them a care-killing competency.\n",
      "   </p>\n",
      "  </div>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup   \n",
    "import requests\n",
    "\n",
    "source = requests.get(\"http://httpbin.org/html\").text\n",
    "# print(source)\n",
    "\n",
    "soup = BeautifulSoup(source,\"lxml\")\n",
    "# print (soup)\n",
    "\n",
    "print (soup.prettify())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (soup.head)\n",
    "print (soup.head.text)\n",
    "\n",
    "print('\\n','*'*20)\n",
    "print (soup.body)\n",
    "\n",
    "print (soup.body.div)\n",
    "print (soup.body.div.p)\n",
    "print (soup.body.div.p.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data from website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the Beautiful soup functions to parse the data returned from the website\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib\n",
    "\n",
    "#specify the url\n",
    "wiki = \"https://en.wikipedia.org/wiki/List_of_state_and_union_territory_capitals_in_India\"\n",
    "\n",
    "source = requests.get(wiki).text\n",
    "'''Or'''\n",
    "source = urllib.request.urlopen(wiki)\n",
    "\n",
    "soup = BeautifulSoup(source,\"lxml\")\n",
    "soup.prettify()\n",
    "# print (soup.prettify())\n",
    "\n",
    "all_tables= soup.find_all('table')\n",
    "# print (all_tables)\n",
    "\n",
    "right_table= soup.find('table', class_='wikitable')  ## use class underscore\n",
    "# print (right_table)\n",
    "\n",
    "#class=\"wikitable sortable plainrowheaders jquery-tablesorter\"\n",
    "#Generate lists\n",
    "A=[]\n",
    "B=[]\n",
    "C=[]\n",
    "D=[]\n",
    "E=[]\n",
    "F=[]\n",
    "\n",
    "for row in right_table.findAll('tr'):\n",
    "    cells = row.findAll('td')\n",
    "    states = row.findAll('th')\n",
    "    if len(cells) == 6:\n",
    "        A.append(states[0].text.strip())\n",
    "        B.append(cells[1].text.strip())\n",
    "        C.append(cells[2].text.strip())\n",
    "        D.append(cells[3].text.strip())\n",
    "        E.append(cells[4].text.strip())\n",
    "        F.append(cells[5].text.strip())\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "col_name = [\"State or UN\",\"Admin Cap\",\"Legis Cap\",\"Judi Cap\",\"Year\",\"Capital\"]\n",
    "col_data = OrderedDict(zip(col_name,[A,B,C,D,E,F]))\n",
    "\n",
    "# If you want to store the data in a csv file\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(col_data) \n",
    "df.to_csv(\"states_table.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scrapping using Selenium for static website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Web Scrapping using Selenium\n",
    "\"\"\"\n",
    "#Download \n",
    "#https://www.seleniumhq.org/download/\n",
    "\n",
    "#installation for firefox\n",
    "#https://github.com/mozilla/geckodriver/\n",
    "\n",
    "#installation for chrome\n",
    "#https://sites.google.com/a/chromium.org/chromedriver/\n",
    "\n",
    "# Add Web Scrapping using Selenium\n",
    "# !pip install selenium\n",
    "\n",
    "\"\"\"\n",
    "Real website data scrapping for Kerela Results\n",
    "\"\"\"\n",
    "\n",
    "from  selenium import webdriver\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup as BS\n",
    "\n",
    "#url = \"http://keralaresults.nic.in/sslc2018rgr8364/swr_sslc.htm\"\n",
    "url = \"http://keralaresults.nic.in/sslc2019duj946/swr_sslc.htm\"\n",
    "\n",
    "\n",
    "#For Windows System\n",
    "browser = webdriver.Chrome(\"E:/Driver/chromedriver.exe\")\n",
    "#browser = webdriver.Firefox(executable_path=\"D:/geckodriver\")\n",
    "\n",
    "# For Mac System\n",
    "#browser = webdriver.Chrome(executable_path=\"/Users/sylvester/chromedriver\")\n",
    "#browser = webdriver.Firefox(executable_path=\"/Users/sylvester/geckodriver\")\n",
    "\n",
    "\n",
    "browser.get(url)\n",
    "sleep(2)\n",
    " \n",
    "school_code = browser.find_element_by_name(\"treg\")\n",
    "\n",
    "#School Code range from 1100 to 5104 \n",
    "school_code.send_keys(\"2000\")\n",
    "sleep(2)\n",
    "\n",
    "\n",
    "#get_school_result = browser.find_element_by_xpath('//*[@id=\"ctrltr\"]/td[3]/input[1]')\n",
    "get_school_result = browser.find_element_by_xpath('/html/body/form/table/tbody/tr[2]/td/table/tbody/tr[3]/td[3]/input[1]')\n",
    "\n",
    "get_school_result.click()\n",
    "sleep(10)\n",
    " \n",
    "html_page = browser.page_source\n",
    "\n",
    "soup = BS(html_page)\n",
    "\n",
    "# Now you can add your logic of reading from BeautifulSoup\n",
    "sleep(10)\n",
    "sleep(10)\n",
    "\n",
    "browser.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Website data scrapping using selenium for dynamic website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Real website data scrapping for List of State and Union Territory using Selenium\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "\n",
    "wiki = \"https://en.wikipedia.org/wiki/List_of_state_and_union_territory_capitals_in_India\"\n",
    "\n",
    "driver = webdriver.Chrome(\"E:/Driver/chromedriver.exe\")\n",
    "\n",
    "#driver = webdriver.Firefox(executable_path=\"/Users/sylvester/geckodriver\")\n",
    "\n",
    "# Opening the submission url\n",
    "driver.get(wiki)\n",
    "\n",
    "right_table= driver.find_element_by_class_name('wikitable')\n",
    "\n",
    "#Generate lists\n",
    "A=[]\n",
    "B=[]\n",
    "C=[]\n",
    "D=[]\n",
    "E=[]\n",
    "F=[]\n",
    "\n",
    "for row in right_table.find_elements_by_tag_name('tr'):\n",
    "    cells = row.find_elements_by_tag_name('td')\n",
    "    states = row.find_elements_by_tag_name('th')\n",
    "    if len(cells) == 6:\n",
    "        A.append(states[0].text.strip())\n",
    "        B.append(cells[1].text.strip())\n",
    "        C.append(cells[2].text.strip())\n",
    "        D.append(cells[3].text.strip())\n",
    "        E.append(cells[4].text.strip())\n",
    "        F.append(cells[5].text.strip())\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "col_name = [\"State or UN\",\"Admin Cap\",\"Legis Cap\",\"Judi Cap\",\"Year\",\"Capital\"]\n",
    "col_data = OrderedDict(zip(col_name,[A,B,C,D,E,F]))\n",
    "\n",
    "df = pd.DataFrame(col_data) \n",
    "df.to_csv(\"states_table.csv\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Code Challenge\n",
    "  Name: \n",
    "    Webscrapping ICC Cricket Page\n",
    "  Filename: \n",
    "    icccricket.py\n",
    "  Problem Statement:\n",
    "    Write a Python code to Scrap data from ICC Ranking's \n",
    "    page and get the ranking table for ODI's (Men). \n",
    "    Create a DataFrame using pandas to store the information.\n",
    "  Hint: \n",
    "    https://www.icc-cricket.com/rankings/mens/team-rankings/odi \n",
    "    \n",
    "    \n",
    "    #https://www.icc-cricket.com/rankings/mens/team-rankings/t20i\n",
    "    #https://www.icc-cricket.com/rankings/mens/team-rankings/test\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Code Challenge:\n",
    "  Name: \n",
    "    Bid Plus\n",
    "  Filename: \n",
    "    bid_plus.py\n",
    "  Problem Statement:\n",
    "      USE SELENIUM\n",
    "      Write a Python code to Scrap data and download data from given url.\n",
    "      url = \"https://bidplus.gem.gov.in/bidlists\"\n",
    "      Make list and append given data:\n",
    "          1. BID NO\n",
    "          2. items\n",
    "          3. Quantity Required\n",
    "          4. Department Name And Address\n",
    "          5. Start Date/Time(Enter date and time in different columns)\n",
    "          6. End Date/Time(Enter date and time in different columns)\n",
    "          \n",
    "          # Optional - Do not do this\n",
    "          7. Name of the PDF file\n",
    "          \n",
    "     Make a csv file add all data in it.\n",
    "      csv Name: bid_plus.csv\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
